{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_date = '2023-03-02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "\n",
      "As a hypothetical expert news aggregator who focuses mainly on banking operations, generate news tags, named entities, sentiment and summary for the following news article. Be honest and output NA if any of the field does not exist or if no news article is given in the input. Use the following format:\n",
      "Tags:\n",
      "Organisation:\n",
      "Named Person:\n",
      "Country:\n",
      "Sentiment (positive/negative/neutral):\n",
      "Concise Summary (without omitting named entities):\n",
      "\n",
      "total number of news articles: 166\n",
      "number of news articles for current run 1\n",
      "number of batches: 0\n",
      "number of extra rows after runs complete, for last batch\n",
      "Created a new dataframe called \"pro_news\" to store values\n",
      "\n",
      "Next part is the CRITICAL PART - note that each run costs $$$, so try to minimise the number of redundant runs\n",
      "If you want to create sample runs, set sample within this block\n"
     ]
    }
   ],
   "source": [
    "# get article embeddings\n",
    "# load libs\n",
    "import tiktoken\n",
    "from openai.embeddings_utils import get_embedding\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# prompt design\n",
    "prompt_gl = \"\\nAs a hypothetical expert news aggregator who focuses mainly on banking operations, generate news tags, named entities, sentiment and summary for the following news article. Be honest and output NA if any of the field does not exist or if no news article is given in the input. Use the following format:\\nTags:\\nOrganisation:\\nNamed Person:\\nCountry:\\nSentiment (positive/negative/neutral):\\nConcise Summary (without omitting named entities):\"\n",
    "print('prompt:\\n' + prompt_gl)\n",
    "\n",
    "\n",
    "# implement retrying library to skip rate limit error\n",
    "import openai  # for OpenAI API calls\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "# news factory function with tenacity backoff implemented\n",
    "def news_factoryv2(news_row):\n",
    "\n",
    "    openai.api_key = \"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\"\n",
    "    \n",
    "    content = news_row.full_text\n",
    "    response = completion_with_backoff(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt= content + prompt_gl,\n",
    "    temperature=0, # to prevent entropy\n",
    "    max_tokens=512,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "    )\n",
    "    text = response['choices'][0]['text'] # get only the text part\n",
    "    #time.sleep(1) # to prevent server error\n",
    "\n",
    "    return text\n",
    "\n",
    "# function to extract details from llm into pd.series \n",
    "def extract_detailsv2(row):\n",
    "    details = {}\n",
    "    for item in row.strip().split('\\n'):\n",
    "        split_item = item.strip().split(':')\n",
    "        if len(split_item) >= 2:\n",
    "            key, value = split_item[:2]\n",
    "            details[key.strip()] = value.strip().strip(\"'\")\n",
    "        else:\n",
    "            print(split_item)\n",
    "            pass\n",
    "    return pd.Series(details)\n",
    "\n",
    "# read headline news 1 feb\n",
    "news_df = pd.read_csv(f'./datasets/ucrawler/{news_date}/{news_date}_rawnews.csv')\n",
    "print('\\ntotal number of news articles: ' + str(len(news_df)))\n",
    "news_df.head(5)\n",
    "\n",
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 3000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# omit reviews that are too long to embed\n",
    "news_df[\"n_tokens\"] = news_df.text.apply(lambda x: len(encoding.encode(x)))\n",
    "news_df = news_df[news_df.n_tokens <= max_tokens]\n",
    "\n",
    "##### set sample when doing test runs  #######\n",
    "n = 1\n",
    "print(f'number of news articles for current run {n}')\n",
    "news_df = news_df.sample(n)\n",
    "\n",
    "runs = len(news_df) // 30\n",
    "extra = len(news_df) % 30\n",
    "print(f'number of batches: {runs}')\n",
    "print(f'number of extra rows after runs complete, for last batch')\n",
    "print('Created a new dataframe called \"pro_news\" to store values')\n",
    "pro_news = pd.DataFrame()\n",
    "print('\\nNext part is the CRITICAL PART - note that each run costs $$$, so try to minimise the number of redundant runs')\n",
    "print('If you want to create sample runs, set sample within this block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hang tight, one more batch to go...\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, int(runs)):\n",
    "    \n",
    "    if i == 0:\n",
    "        word = 'first'\n",
    "    else:\n",
    "        word = 'next'\n",
    "    \n",
    "    print(f'running {word} batch.... ' + str(i*30) + ' to ' + str((i+1)*30) + '...')\n",
    "    \n",
    "    batch = news_df.iloc[(i*30):(i+1)*30]\n",
    "    # Apply the factory function to each row in the batch\n",
    "    batch['gpt_out'] = batch.apply(news_factoryv2, axis=1)\n",
    "\n",
    "    pro_news = pd.concat([pro_news, batch])\n",
    "    # Wait for 1 minute before sending the next batch\n",
    "    time.sleep(60)\n",
    "\n",
    "    print('success!')\n",
    "\n",
    "#final ones\n",
    "print('hang tight, one more batch to go...')\n",
    "batch = news_df.iloc[(runs*30):len(news_df)]\n",
    "# Apply the factory function to each row in the batch\n",
    "batch['gpt_out'] = batch.apply(news_factoryv2, axis=1)\n",
    "pro_news = pd.concat([pro_news, batch])\n",
    "\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>pub_time</th>\n",
       "      <th>meta_images</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>full_text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>gpt_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>China's CATL sells its stake in Australian lit...</td>\n",
       "      <td>FILE PHOTO: The logo of Chinese battery maker ...</td>\n",
       "      <td>2023-03-02 01:32:43.754000+00:00</td>\n",
       "      <td>https://onecms-res.cloudinary.com/image/upload...</td>\n",
       "      <td>CNA</td>\n",
       "      <td>https://www.channelnewsasia.com/business/china...</td>\n",
       "      <td>China's CATL sells its stake in Australian lit...</td>\n",
       "      <td>177</td>\n",
       "      <td>\\n\\nTags: CATL, Pilbara Minerals Ltd, Goldman ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "116  China's CATL sells its stake in Australian lit...   \n",
       "\n",
       "                                                  text  \\\n",
       "116  FILE PHOTO: The logo of Chinese battery maker ...   \n",
       "\n",
       "                             pub_time  \\\n",
       "116  2023-03-02 01:32:43.754000+00:00   \n",
       "\n",
       "                                           meta_images source  \\\n",
       "116  https://onecms-res.cloudinary.com/image/upload...    CNA   \n",
       "\n",
       "                                                   url  \\\n",
       "116  https://www.channelnewsasia.com/business/china...   \n",
       "\n",
       "                                             full_text  n_tokens  \\\n",
       "116  China's CATL sells its stake in Australian lit...       177   \n",
       "\n",
       "                                               gpt_out  \n",
       "116  \\n\\nTags: CATL, Pilbara Minerals Ltd, Goldman ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of news articles: 1\n",
      "extracting generated fields...\n",
      "saving dataset...\n"
     ]
    }
   ],
   "source": [
    "print('Number of news articles: ' + str(len(pro_news)))\n",
    "\n",
    "# store back-up\n",
    "back_news = pro_news\n",
    "\n",
    "# extract details from column gpt_out and store into new columns\n",
    "print('extracting generated fields...')\n",
    "df_details = pro_news['gpt_out'].apply(extract_detailsv2)\n",
    "\n",
    "df_key = df_details[['Tags', 'Organisation', 'Named Person', 'Country', 'Sentiment', 'Concise Summary']]\n",
    "pro_news = pd.concat([pro_news, df_key], axis=1)\n",
    "\n",
    "# change some column name\n",
    "pro_news.rename(columns={'Concise Summary':'Summary'}, inplace=True)\n",
    "\n",
    "# save dataset\n",
    "print('saving dataset...')\n",
    "path = f'./datasets/ucrawler/{news_date}/{news_date}_ucgpt.csv'\n",
    "pro_news.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>pub_time</th>\n",
       "      <th>meta_images</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>full_text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>gpt_out</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Organisation</th>\n",
       "      <th>Named Person</th>\n",
       "      <th>Country</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>China's CATL sells its stake in Australian lit...</td>\n",
       "      <td>FILE PHOTO: The logo of Chinese battery maker ...</td>\n",
       "      <td>2023-03-02 01:32:43.754000+00:00</td>\n",
       "      <td>https://onecms-res.cloudinary.com/image/upload...</td>\n",
       "      <td>CNA</td>\n",
       "      <td>https://www.channelnewsasia.com/business/china...</td>\n",
       "      <td>China's CATL sells its stake in Australian lit...</td>\n",
       "      <td>177</td>\n",
       "      <td>\\n\\nTags: CATL, Pilbara Minerals Ltd, Goldman ...</td>\n",
       "      <td>CATL, Pilbara Minerals Ltd, Goldman Sachs, UBS</td>\n",
       "      <td>CATL, Pilbara Minerals Ltd, Goldman Sachs, UBS</td>\n",
       "      <td>NA</td>\n",
       "      <td>China, Australia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>CATL, the world's largest battery maker, has s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "116  China's CATL sells its stake in Australian lit...   \n",
       "\n",
       "                                                  text  \\\n",
       "116  FILE PHOTO: The logo of Chinese battery maker ...   \n",
       "\n",
       "                             pub_time  \\\n",
       "116  2023-03-02 01:32:43.754000+00:00   \n",
       "\n",
       "                                           meta_images source  \\\n",
       "116  https://onecms-res.cloudinary.com/image/upload...    CNA   \n",
       "\n",
       "                                                   url  \\\n",
       "116  https://www.channelnewsasia.com/business/china...   \n",
       "\n",
       "                                             full_text  n_tokens  \\\n",
       "116  China's CATL sells its stake in Australian lit...       177   \n",
       "\n",
       "                                               gpt_out  \\\n",
       "116  \\n\\nTags: CATL, Pilbara Minerals Ltd, Goldman ...   \n",
       "\n",
       "                                               Tags  \\\n",
       "116  CATL, Pilbara Minerals Ltd, Goldman Sachs, UBS   \n",
       "\n",
       "                                       Organisation Named Person  \\\n",
       "116  CATL, Pilbara Minerals Ltd, Goldman Sachs, UBS           NA   \n",
       "\n",
       "              Country Sentiment  \\\n",
       "116  China, Australia  Positive   \n",
       "\n",
       "                                               Summary  \n",
       "116  CATL, the world's largest battery maker, has s...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def thing_counter(thing):\n",
    "\n",
    "    pro_news[thing].fillna(\"\", inplace=True)\n",
    "\n",
    "    country_sr = pro_news[thing].apply(lambda x: x.split(','))\n",
    "\n",
    "    # Convert the 'Country' column into a list\n",
    "    country_list = country_sr.tolist()\n",
    "    #print(country_list)\n",
    "    #print(len(country_list))\n",
    "    # Flatten the list of lists into a single list using a list comprehension\n",
    "    flat_list = [item for sublist in country_list for item in sublist]\n",
    "    \n",
    "    #print(flat_list)\n",
    "    if ' Ltd.' in flat_list:\n",
    "        flat_list.remove(' Ltd.')\n",
    "\n",
    "    if 'Ltd.' in flat_list:\n",
    "        flat_list.remove('Ltd.')\n",
    "\n",
    "    if 'N/A' in flat_list:\n",
    "        flat_list.remove('N/A')\n",
    "\n",
    "# follow this script to standardise multiple spellings of the same word\n",
    "    flat_list = ['United States' if x == 'US' else x for x in flat_list]\n",
    "    flat_list = ['United States' if x == ' US' else x for x in flat_list]\n",
    "    flat_list = ['COVID-19' if x == 'Covid-19' else x for x in flat_list]\n",
    "    flat_list = ['COVID-19' if x == ' Covid-19' else x for x in flat_list]\n",
    "\n",
    "    if 'NA' in flat_list:\n",
    "        flat_list.remove('NA')\n",
    "\n",
    "\n",
    "    # Remove the surrounding white spaces from the list\n",
    "    stripped_countries = [country.strip() for country in flat_list]\n",
    "    #print(stripped_countries)\n",
    "    # Create a frequency count of the countries\n",
    "    country_counts = dict(collections.Counter(stripped_countries))\n",
    "\n",
    "    # Create a new dataframe with the country and its count\n",
    "    new_df = pd.DataFrame({thing: list(country_counts.keys()), 'Count': list(country_counts.values())})\n",
    "\n",
    "    new_df = new_df\n",
    "    new_df = new_df[new_df[thing] != '']\n",
    "    new_df = new_df[new_df[thing] != 'NA']\n",
    "    new_df = new_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    new_df.Count = new_df['Count'].apply(lambda x: str(x))\n",
    "    new_df[thing + '_count'] = new_df[thing] + ' (' + new_df['Count'] + ')'\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Count</th>\n",
       "      <th>Country_count</th>\n",
       "      <th>Organisation</th>\n",
       "      <th>Count</th>\n",
       "      <th>Organisation_count</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Count</th>\n",
       "      <th>Tags_count</th>\n",
       "      <th>date</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>1</td>\n",
       "      <td>China (1)</td>\n",
       "      <td>CATL</td>\n",
       "      <td>1</td>\n",
       "      <td>CATL (1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia (1)</td>\n",
       "      <td>Pilbara Minerals Ltd</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilbara Minerals Ltd (1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Goldman Sachs</td>\n",
       "      <td>1</td>\n",
       "      <td>Goldman Sachs (1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UBS</td>\n",
       "      <td>1</td>\n",
       "      <td>UBS (1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>All</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>All</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>tags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country Count  Country_count          Organisation Count  \\\n",
       "0      China     1      China (1)                  CATL     1   \n",
       "1  Australia     1  Australia (1)  Pilbara Minerals Ltd     1   \n",
       "2        NaN   NaN            NaN         Goldman Sachs     1   \n",
       "3        NaN   NaN            NaN                   UBS     1   \n",
       "4        All    NA             NA                   All    NA   \n",
       "\n",
       "         Organisation_count Tags Count Tags_count       date  file  \n",
       "0                  CATL (1)  NaN   NaN        NaN 2023-03-02  tags  \n",
       "1  Pilbara Minerals Ltd (1)  NaN   NaN        NaN 2023-03-02  tags  \n",
       "2         Goldman Sachs (1)  NaN   NaN        NaN 2023-03-02  tags  \n",
       "3                   UBS (1)  NaN   NaN        NaN 2023-03-02  tags  \n",
       "4                        NA  All    NA         NA 2023-03-02  tags  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count countries\n",
    "country_df = thing_counter('Country')\n",
    "country_df = country_df.reset_index(drop=True)\n",
    "country_sm = country_df[0:10]\n",
    "\n",
    "# count organisations\n",
    "org_df = thing_counter('Organisation')\n",
    "org_df = org_df.reset_index(drop=True)\n",
    "org_sm = org_df[0:10]\n",
    "\n",
    "# count keywords\n",
    "keywords_df = thing_counter('Tags')\n",
    "\n",
    "# remove keywords that have already been captured in country and organisation\n",
    "keywords_torem = country_df['Country'].to_list() + org_df['Organisation'].to_list()\n",
    "# Use the isin function to remove the rows in keywords that contain keywords in the list\n",
    "keywords_un = keywords_df[~keywords_df['Tags'].isin(keywords_torem)]\n",
    "# other adhoc keyword removal\n",
    "keywords_un = keywords_un[keywords_un.Tags != 'involuntary manslaughter']\n",
    "keywords_un.reset_index(drop=True, inplace=True)\n",
    "keywords_sm = keywords_un[0:10]\n",
    "\n",
    "# concatenate all dfs\n",
    "tags_df = pd.concat([country_sm, org_sm, keywords_sm], axis=1)\n",
    "\n",
    "# add 'All' column to have it as a parameter field in tableau\n",
    "list_row = ['All', 'NA', 'NA', 'All', 'NA', 'NA', 'All', 'NA', 'NA']\n",
    "tags_df.loc[len(tags_df)] = list_row\n",
    "\n",
    "# add date and file name\n",
    "tags_df['date'] = news_date\n",
    "tags_df['file'] = 'tags'\n",
    "\n",
    "tags_df.date = pd.to_datetime(tags_df.date)\n",
    "\n",
    "tags_df.to_excel(f'./datasets/ucrawler/{news_date}/{news_date}_newstags.xlsx', index=False)\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_alert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
