{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_date = '2023-02-28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article embeddings\n",
    "# load libs\n",
    "import tiktoken\n",
    "from openai.embeddings_utils import get_embedding\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# prompt design\n",
    "prompt_gl = \"\\nAs a hypothetical expert news aggregator who focuses mainly on banking operations, generate news tags, named entities, sentiment and summary for the following news article. Be honest and output NA if any of the field does not exist or if no news article is given in the input. Use the following format:\\nTags:\\nOrganisation:\\nNamed Person:\\nCountry:\\nSentiment (positive/negative/neutral):\\nConcise Summary (without omitting named entities):\"\n",
    "print('prompt:\\n' + prompt_gl)\n",
    "\n",
    "\n",
    "# implement retrying library to skip rate limit error\n",
    "import openai  # for OpenAI API calls\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "# news factory function with tenacity backoff implemented\n",
    "def news_factoryv2(news_row):\n",
    "\n",
    "    openai.api_key = \"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\"\n",
    "    \n",
    "    content = news_row.full_text\n",
    "    response = completion_with_backoff(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt= content + prompt_gl,\n",
    "    temperature=0, # to prevent entropy\n",
    "    max_tokens=512,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "    )\n",
    "    text = response['choices'][0]['text'] # get only the text part\n",
    "    #time.sleep(1) # to prevent server error\n",
    "\n",
    "    return text\n",
    "\n",
    "# function to extract details from llm into pd.series \n",
    "def extract_detailsv2(row):\n",
    "    details = {}\n",
    "    for item in row.strip().split('\\n'):\n",
    "        split_item = item.strip().split(':')\n",
    "        if len(split_item) >= 2:\n",
    "            key, value = split_item[:2]\n",
    "            details[key.strip()] = value.strip().strip(\"'\")\n",
    "        else:\n",
    "            print(split_item)\n",
    "            pass\n",
    "    return pd.Series(details)\n",
    "\n",
    "# read headline news 1 feb\n",
    "news_df = pd.read_csv(f'./datasets/ucrawler/{news_date}/rawnews_{news_date}.csv')\n",
    "print('\\ntotal number of news articles: ' + str(len(news_df)))\n",
    "news_df.head(5)\n",
    "\n",
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 3000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# omit reviews that are too long to embed\n",
    "news_df[\"n_tokens\"] = news_df.text.apply(lambda x: len(encoding.encode(x)))\n",
    "news_df = news_df[news_df.n_tokens <= max_tokens]\n",
    "\n",
    "##### set sample when doing test runs  #######\n",
    "n = 1\n",
    "print(f'number of news articles for current run {n}')\n",
    "news_df = news_df.sample(n)\n",
    "\n",
    "runs = len(news_df) // 30\n",
    "extra = len(news_df) % 30\n",
    "print(f'number of batches: {runs}')\n",
    "print(f'number of extra rows after runs complete, for last batch')\n",
    "print('Created a new dataframe called \"pro_news\" to store values')\n",
    "pro_news = pd.DataFrame()\n",
    "print('\\nNext part is the CRITICAL PART - note that each run costs $$$, so try to minimise the number of redundant runs')\n",
    "print('If you want to create sample runs, set sample within this block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, int(runs)):\n",
    "    \n",
    "    if i == 0:\n",
    "        word = 'first'\n",
    "    else:\n",
    "        word = 'next'\n",
    "    \n",
    "    print(f'running {word} batch.... ' + str(i*30) + ' to ' + str((i+1)*30) + '...')\n",
    "    \n",
    "    batch = news_df.iloc[(i*30):(i+1)*30]\n",
    "    # Apply the factory function to each row in the batch\n",
    "    batch['gpt_out'] = batch.apply(news_factory, axis=1)\n",
    "\n",
    "    pro_news = pd.concat([pro_news, batch])\n",
    "    # Wait for 1 minute before sending the next batch\n",
    "    time.sleep(60)\n",
    "\n",
    "    print('success!')\n",
    "\n",
    "#final ones\n",
    "print('hang tight, one more batch to go...')\n",
    "batch = news_df.iloc[(runs*30):len(news_df)]\n",
    "# Apply the factory function to each row in the batch\n",
    "batch['gpt_out'] = batch.apply(news_factoryv2, axis=1)\n",
    "pro_news = pd.concat([pro_news, batch])\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of news articles: ' + str(len(pro_news)))\n",
    "\n",
    "# store back-up\n",
    "back_news = pro_news\n",
    "\n",
    "# extract details from column gpt_out and store into new columns\n",
    "print('extracting generated fields...')\n",
    "df_details = pro_news['gpt_out'].apply(extract_detailsv2)\n",
    "\n",
    "df_key = df_details[['Tags', 'Organisation', 'Named Person', 'Country', 'Sentiment', 'Concise Summary']]\n",
    "pro_news = pd.concat([pro_news, df_key], axis=1)\n",
    "\n",
    "# save dataset\n",
    "print('saving dataset...')\n",
    "path = f'./datasets/ucrawler/{news_date}/{news_date}_ucgpt.csv'\n",
    "pro_news.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def thing_counter(thing):\n",
    "\n",
    "    pro_news[thing].fillna(\"\", inplace=True)\n",
    "\n",
    "    country_sr = pro_news[thing].apply(lambda x: x.split(','))\n",
    "\n",
    "    # Convert the 'Country' column into a list\n",
    "    country_list = country_sr.tolist()\n",
    "    #print(country_list)\n",
    "    #print(len(country_list))\n",
    "    # Flatten the list of lists into a single list using a list comprehension\n",
    "    flat_list = [item for sublist in country_list for item in sublist]\n",
    "    \n",
    "    #print(flat_list)\n",
    "    if ' Ltd.' in flat_list:\n",
    "        flat_list.remove(' Ltd.')\n",
    "\n",
    "    if 'Ltd.' in flat_list:\n",
    "        flat_list.remove('Ltd.')\n",
    "\n",
    "    if 'N/A' in flat_list:\n",
    "        flat_list.remove('N/A')\n",
    "\n",
    "# follow this script to standardise multiple spellings of the same word\n",
    "    flat_list = ['United States' if x == 'US' else x for x in flat_list]\n",
    "    flat_list = ['United States' if x == ' US' else x for x in flat_list]\n",
    "    flat_list = ['COVID-19' if x == 'Covid-19' else x for x in flat_list]\n",
    "    flat_list = ['COVID-19' if x == ' Covid-19' else x for x in flat_list]\n",
    "\n",
    "    if 'NA' in flat_list:\n",
    "        flat_list.remove('NA')\n",
    "\n",
    "\n",
    "    # Remove the surrounding white spaces from the list\n",
    "    stripped_countries = [country.strip() for country in flat_list]\n",
    "    #print(stripped_countries)\n",
    "    # Create a frequency count of the countries\n",
    "    country_counts = dict(collections.Counter(stripped_countries))\n",
    "\n",
    "    # Create a new dataframe with the country and its count\n",
    "    new_df = pd.DataFrame({thing: list(country_counts.keys()), 'Count': list(country_counts.values())})\n",
    "\n",
    "    new_df = new_df\n",
    "    new_df = new_df[new_df[thing] != '']\n",
    "    new_df = new_df[new_df[thing] != 'NA']\n",
    "    new_df = new_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    new_df.Count = new_df['Count'].apply(lambda x: str(x))\n",
    "    new_df[thing + '_count'] = new_df[thing] + ' (' + new_df['Count'] + ')'\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count countries\n",
    "country_df = thing_counter('Country')\n",
    "country_df = country_df.reset_index(drop=True)\n",
    "country_sm = country_df[0:10]\n",
    "\n",
    "# count organisations\n",
    "org_df = thing_counter('Organisation')\n",
    "org_df = org_df.reset_index(drop=True)\n",
    "org_sm = org_df[0:10]\n",
    "\n",
    "# count keywords\n",
    "keywords_df = thing_counter('Tags')\n",
    "\n",
    "# remove keywords that have already been captured in country and organisation\n",
    "keywords_torem = country_df['Country'].to_list() + org_df['Organisation'].to_list()\n",
    "# Use the isin function to remove the rows in keywords that contain keywords in the list\n",
    "keywords_un = keywords_df[~keywords_df['Tags'].isin(keywords_torem)]\n",
    "# other adhoc keyword removal\n",
    "keywords_un = keywords_un[keywords_un.Tags != 'involuntary manslaughter']\n",
    "keywords_un.reset_index(drop=True, inplace=True)\n",
    "keywords_sm = keywords_un[0:10]\n",
    "\n",
    "# concatenate all dfs\n",
    "tags_df = pd.concat([country_sm, org_sm, keywords_sm], axis=1)\n",
    "\n",
    "# add 'All' column to have it as a parameter field in tableau\n",
    "list_row = ['All', 'NA', 'NA', 'All', 'NA', 'NA', 'All', 'NA', 'NA']\n",
    "tags_df.loc[len(tags_df)] = list_row\n",
    "\n",
    "# add date and file name\n",
    "tags_df['date'] = news_date\n",
    "tags_df['file'] = 'tags'\n",
    "\n",
    "tags_df.date = pd.to_datetime(tags_df.date)\n",
    "\n",
    "tags_df.to_excel(f'./datasets/ucrawler/{news_date}/{news_date}_newstags.xlsx', index=False)\n",
    "tags_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
